{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See utils.py for the imports\n",
    "\n",
    "from utils import *\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import timeit\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#%load_ext line_profiler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Where to save the figures (adapted this from https://github.com/ageron/handson-ml2)\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_ID = \"stock_movement_tweet_data_wrangling\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", PROJECT_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=72):\n",
    "    \"\"\" \n",
    "    resolution quality\n",
    "    300 high \n",
    "    150 medium\n",
    "    72 low\n",
    "    \"\"\"\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweets_user(df):\n",
    "    # convert dates to datetime and split\n",
    "    datetimes = pd.to_datetime(df['created_at'])\n",
    "    df['time'], df['date'] = datetimes.dt.time, datetimes.dt.date\n",
    "    \n",
    "    # get open and closing times\n",
    "    open_t = dt.strptime('09:30:00', '%H:%M:%S').time()\n",
    "    close_t = dt.strptime('16:00:00', '%H:%M:%S').time()\n",
    "    \n",
    "    # filter dataframes based off when tweet occurs\n",
    "    before_df = df[df['time'] < open_t]\n",
    "    during_df = df[((df['time'] >= open_t) & (df['time'] < close_t))]\n",
    "    after_df = df[df['time'] >= close_t]\n",
    "    \n",
    "    # reset tweet times to next open or close time\n",
    "    before_df['time'] = open_t\n",
    "    during_df['time'] = close_t\n",
    "    \n",
    "    # if tweet occurs after close, set it to next day and set time to open\n",
    "    after_df['date'] += timedelta(days=1)\n",
    "    after_df['time'] = open_t\n",
    "\n",
    "    # reassemble dataframes with updated dates/times\n",
    "    df = before_df.append(during_df).append(after_df)\n",
    "    \n",
    "    # sort by date and time\n",
    "    df['date'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str))\n",
    "    df = df.sort_values(by='date', ascending=True)\n",
    "    \n",
    "    # collect tweets in a dictionary (key:value ->  Date : [tweets])\n",
    "    collected_tweets = {}\n",
    "    tweet = []\n",
    "    combined_df = pd.DataFrame(index=sorted(list(set(df['date']))))\n",
    "    combined_df['tweet'] = [[] for _ in range(len(combined_df))]\n",
    "    \n",
    "    # iterate over dataframe and add tweets to matching dates\n",
    "    for i in range(len(combined_df)):\n",
    "        time = combined_df.index[i]\n",
    "        matching_dates = df[df['date'] == time]\n",
    "        for tweet in matching_dates['tweet']:\n",
    "            combined_df['tweet'].iloc[i].append(tweet)\n",
    "     \n",
    "    # add number of tweets and username and return\n",
    "    combined_df['num_tweets'] = combined_df['tweet'].apply(lambda x: len(x))\n",
    "    combined_df['username'] = df['username'].unique()[0]\n",
    "    return combined_df\n",
    "\n",
    "def combine_tweets(full_df, verbose=False):\n",
    "    \"\"\"\n",
    "    group tweets together by next open/close date\n",
    "    \n",
    "    Arguments:\n",
    "    full_df -- A dataframe of users tweets and created_at dates. \n",
    "    \n",
    "    Returns:\n",
    "    merged_df -- A dataframe of grouped user tweets as lists before the next open/close datetime\n",
    "  \n",
    "    A tweet is grouped via the following criteria:\n",
    "    market_open = 09:30:00 EST\n",
    "    market_close = 16:00:00 EST\n",
    "    \n",
    "    - If tweet created_at occurs before market_open on same date, it is grouped as before_market_tweets. \n",
    "    - It tweet is on or after market_open but before market_close, (i.e. during market hours) it is grouped as during_market_tweets. \n",
    "    - If tweet is on or after market_close, it is grouped as after_market_tweets.\n",
    "    \"\"\"\n",
    "    \n",
    "    users = full_df.username.unique()\n",
    "    merged_df = pd.DataFrame()\n",
    "    if verbose:\n",
    "        max_length = 11963\n",
    "        start = timeit.default_timer()\n",
    "        print('')\n",
    "        print('digesting dataframes....')\n",
    "        print('---{:.2f} % complete -------'.format(0.0))\n",
    "    for user in users:\n",
    "        merged_df = merged_df.append(combine_tweets_user(full_df[full_df['username'] == user]))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('--{:.2f} % complete -------'.format(100*len(merged_df) / max_length))\n",
    "    if verbose:\n",
    "        stop = timeit.default_timer()\n",
    "        print('digestion completed ---- runtime {:.2f} seconds'.format(stop - start))\n",
    "    return merged_df.reset_index(drop=False).rename(columns={'index':'date'})\n",
    "    \n",
    "def combine_tweets_stocks(ceos_merged, stocks_full):\n",
    "    new_df = pd.DataFrame()\n",
    "    for user in ceos_merged['username'].unique():\n",
    "        ticker = handles_tickers[user]\n",
    "        tweet_df = ceos_merged[ceos_merged['username'] == user]\n",
    "        stock_df = stocks_full[stocks_full['ticker'] == ticker]\n",
    "        new_df = new_df.append(tweet_df.merge(stock_df, how='left', on='date')).dropna(subset=['ticker'])  \n",
    "    return new_df\n",
    "\n",
    "def fix_closed_market_tweets(test):\n",
    "    test = test.dropna()\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "    test = test.set_index('date')\n",
    "    for i in range(len(test)):\n",
    "        test['tweet'].iloc[i] += \" \"\n",
    "\n",
    "    i = 0\n",
    "    combined=pd.DataFrame()\n",
    "\n",
    "    while i < len(test):\n",
    "        if test['price'].iloc[i] == 0:\n",
    "            combined = test.iloc[i]\n",
    "            j = i + 1\n",
    "            while test['price'].iloc[j] == 0:\n",
    "                combined += test.iloc[j]\n",
    "                j += 1\n",
    "            username = test.iloc[j]['username']\n",
    "            price = test.iloc[j]['price']\n",
    "            percent_change = test.iloc[j]['percent change']\n",
    "            test.iloc[j] += combined\n",
    "            test['username'].iloc[j] = username\n",
    "            test['price'].iloc[j] = price\n",
    "            test['percent change'].iloc[j] = percent_change\n",
    "            i = j\n",
    "        i += 1\n",
    "    test = test[test['price'] !=0]\n",
    "    return test\n",
    "\n",
    "def organize_stocks(df): \n",
    "    \"\"\"\n",
    "    Combines the open/close dates and prices into a single dataframe\n",
    "    \"\"\"\n",
    "    stock_open = pd.DataFrame()\n",
    "    stock_close = pd.DataFrame()\n",
    "\n",
    "    stock_open['date'] = pd.to_datetime(df['date'].dt.strftime('%Y-%m-%d 09:30:00'))\n",
    "    stock_close['date'] = pd.to_datetime(df['date'].dt.strftime('%Y-%m-%d 16:00:00'))\n",
    "\n",
    "    stock_open['price'] = df['open']\n",
    "    stock_close['price'] = df['close']\n",
    "    \n",
    "    start_date_open = dt.strftime(stock_open.date.min(), '%Y-%m-%d %H:%M:%S')\n",
    "    start_date_close = dt.strftime(stock_close.date.min(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    end_date_open = dt.strftime(stock_open.date.max(), '%Y-%m-%d %H:%M:%S')\n",
    "    end_date_close = dt.strftime(stock_close.date.max(), '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    date_indx_open = pd.Series(pd.date_range(start_date_open, end_date_open), name='date')\n",
    "    date_indx_close = pd.Series(pd.date_range(start_date_close, end_date_close), name='date')\n",
    "    \n",
    "    stock_open = pd.merge(date_indx_open, stock_open, how='left')\n",
    "    stock_close = pd.merge(date_indx_close, stock_close, how='left')\n",
    "    \n",
    "    stock = pd.concat([stock_open, stock_close])\n",
    "    return stock.sort_values(by='date', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect the stock data from yahoo Finance and organize a dictionary of dataframes with the open/close prices combined. \n",
    "usernames = ['elonmusk', 'levie', 'jack', 'Benioff','richardbranson', 'JohnLegere']\n",
    "stock_names = ['TSLA','BOX', 'TWTR','CRM','SPCE', 'TMUS']\n",
    "user_stock_mapping = dict(zip(usernames, stock_names))\n",
    "stocks = {stock:pd.read_pickle(f'data/{stock}.pkl') for stock in stock_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get needed columns and change the date to lowercase\n",
    "for stock in stocks:\n",
    "    stocks[stock].reset_index(inplace=True)\n",
    "    stocks[stock] =  stocks[stock][['Date','Open', 'Close']]\n",
    "    stocks[stock].columns = stocks[stock].columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stocks:\n",
    "    stocks[stock] = organize_stocks(stocks[stock])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-trending the time series stock data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the percent change in the stock prices as a target that we will later bin into categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a percent change column from the price feature\n",
    "for stock in stocks:\n",
    "    stocks[stock]['percent change'] = stocks[stock]['price'].pct_change()\n",
    "    stocks[stock].fillna(0, inplace=True)\n",
    "    \n",
    "# convert to dataframe\n",
    "stocks_full = pd.DataFrame()\n",
    "for stock in stocks:\n",
    "    # get stock ticker for grouping\n",
    "    stocks[stock]['ticker'] = stock\n",
    "    stocks_full = stocks_full.append(stocks[stock])\n",
    "\n",
    "# pickle and save\n",
    "#pd.to_pickle(stocks_full, './data/stocks_full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Collecting the tweets <a id='1.2.5_Collecting_Tweet'></a>\n",
    "We will collect the CEOs tweets over the same time-span as the collected stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets from the CEOs of the companies\n",
    "        \n",
    "handles_tickers = {'elonmusk':'TSLA', 'levie':'BOX','jack':'TWTR', 'Benioff':'CRM', \n",
    "            'richardbranson':'SPCE', 'JohnLegere':'TMUS'}\n",
    "\n",
    "start_date = {}\n",
    "for user in handles_tickers:\n",
    "    start_date[user] = dt.strftime(stocks[handles_tickers[user]].reset_index().date.min(), '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "ceos = pd.read_pickle('./data/ceos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "digesting dataframes....\n",
      "---0.00 % complete -------\n",
      "\n",
      "--18.38 % complete -------\n",
      "\n",
      "--26.44 % complete -------\n",
      "\n",
      "--38.93 % complete -------\n",
      "\n",
      "--54.63 % complete -------\n",
      "\n",
      "--71.44 % complete -------\n",
      "\n",
      "--100.00 % complete -------\n",
      "digestion completed ---- runtime 66.36 seconds\n"
     ]
    }
   ],
   "source": [
    "ceos_merged = combine_tweets(ceos, verbose=True)\n",
    "wrangled_df = combine_tweets_stocks(ceos_merged, stocks_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22    [Journalist Q&amp;A for 30 mins and embargo en...\n",
       "22    [7,000 people registered for BoxWorks this wee...\n",
       "22    [When was the last time you tried something fo...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangled_df.loc[22]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweets_stocks(ceos_merged, stocks_full):\n",
    "    new_df = pd.DataFrame()\n",
    "    for user in ceos_merged['username'].unique():\n",
    "        ticker = handles_tickers[user]\n",
    "        tweet_df = ceos_merged[ceos_merged['username'] == user]\n",
    "        stock_df = stocks_full[stocks_full['ticker'] == ticker]\n",
    "        \n",
    "        # drop na values where \n",
    "        new_df = new_df.append(tweet_df.merge(stock_df, how='left', on='date')).dropna(subset=['ticker'])  \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangled_df[wrangled_df['username'] == 'elonmusk'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>username</th>\n",
       "      <th>price</th>\n",
       "      <th>percent change</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-07-16 16:00:00</th>\n",
       "      <td>[@TeslaNY Do you even press?, @Teslarati Impro...</td>\n",
       "      <td>4</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>644.219971</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-17 09:30:00</th>\n",
       "      <td>[@fael097 Pure coincidence!, @ValaAfshar Even ...</td>\n",
       "      <td>9</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-17 16:00:00</th>\n",
       "      <td>[@SamTwits Nice,  https://t.co/d4ZOSKZESP, Tap...</td>\n",
       "      <td>12</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-18 09:30:00</th>\n",
       "      <td>[@ArtifactsHub And all-time hodl champion, @Ar...</td>\n",
       "      <td>6</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-18 16:00:00</th>\n",
       "      <td>[@thePiggsBoson Problem 1st, theory 2nd is for...</td>\n",
       "      <td>1</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-19 09:30:00</th>\n",
       "      <td>[@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...</td>\n",
       "      <td>3</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>629.890015</td>\n",
       "      <td>-0.022244</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 tweet  \\\n",
       "date                                                                     \n",
       "2021-07-16 16:00:00  [@TeslaNY Do you even press?, @Teslarati Impro...   \n",
       "2021-07-17 09:30:00  [@fael097 Pure coincidence!, @ValaAfshar Even ...   \n",
       "2021-07-17 16:00:00  [@SamTwits Nice,  https://t.co/d4ZOSKZESP, Tap...   \n",
       "2021-07-18 09:30:00  [@ArtifactsHub And all-time hodl champion, @Ar...   \n",
       "2021-07-18 16:00:00  [@thePiggsBoson Problem 1st, theory 2nd is for...   \n",
       "2021-07-19 09:30:00  [@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...   \n",
       "\n",
       "                     num_tweets  username       price  percent change ticker  \n",
       "date                                                                          \n",
       "2021-07-16 16:00:00           4  elonmusk  644.219971       -0.015977   TSLA  \n",
       "2021-07-17 09:30:00           9  elonmusk    0.000000        0.000000   TSLA  \n",
       "2021-07-17 16:00:00          12  elonmusk    0.000000        0.000000   TSLA  \n",
       "2021-07-18 09:30:00           6  elonmusk    0.000000        0.000000   TSLA  \n",
       "2021-07-18 16:00:00           1  elonmusk    0.000000        0.000000   TSLA  \n",
       "2021-07-19 09:30:00           3  elonmusk  629.890015       -0.022244   TSLA  "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('date')['2021-07-16 16:00:00': '2021-07-19 09:30:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('date')\n",
    "combined = pd.DataFrame()\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    \n",
    "    # find where price is zero (saturday, sunday, or holiday)\n",
    "    if df['price'].iloc[i] == 0:\n",
    "        combined = df[['tweet', 'num_tweets']].iloc[i]\n",
    "        \n",
    "        # iterate through dataframe until next non-zero price entry\n",
    "        j = i + 1\n",
    "        while df['price'].iloc[j] == 0:\n",
    "            combined += df[['tweet', 'num_tweets']].iloc[j]\n",
    "            j += 1\n",
    "        username = df.iloc[j]['username']\n",
    "        price = df.iloc[j]['price']\n",
    "        percent_change = df.iloc[j]['percent change']\n",
    "        df.iloc[j] += combined\n",
    "        df['username'].iloc[j] = username\n",
    "        df['price'].iloc[j] = price\n",
    "        df['percent change'].iloc[j] = percent_change\n",
    "        i = j\n",
    "    i += 1\n",
    "df = df[df['price'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>username</th>\n",
       "      <th>price</th>\n",
       "      <th>percent change</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-08-23 16:00:00</th>\n",
       "      <td>[Journalist Q&amp;amp;A for 30 mins and embargo en...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>44.967999</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-30 16:00:00</th>\n",
       "      <td>[Thanks for the longstanding faith in SpaceX. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>42.268002</td>\n",
       "      <td>-0.022072</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31 16:00:00</th>\n",
       "      <td>[@Lockyep Not allowed, according to HK regulat...</td>\n",
       "      <td>5</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>42.402000</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01 16:00:00</th>\n",
       "      <td>[Loss of Falcon vehicle today during propellan...</td>\n",
       "      <td>1</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>40.153999</td>\n",
       "      <td>-0.039424</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-02 09:30:00</th>\n",
       "      <td>[Finishing Autopilot blog postponed to  end of...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>40.466000</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-16 09:30:00</th>\n",
       "      <td>[@AaronS5_ @FrenchieEAP @karpathy Yes, @Austin...</td>\n",
       "      <td>4</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>654.679993</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-16 16:00:00</th>\n",
       "      <td>[@TeslaNY Do you even press?, @Teslarati Impro...</td>\n",
       "      <td>4</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>644.219971</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-19 09:30:00</th>\n",
       "      <td>[@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...</td>\n",
       "      <td>31</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>629.890015</td>\n",
       "      <td>-0.022244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-19 16:00:00</th>\n",
       "      <td>[@jack @BitcoinMagazine @CathieDWood Sure, I h...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>646.219971</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 09:30:00</th>\n",
       "      <td>[@TLPN_Official @SpaceX Depending on progress ...</td>\n",
       "      <td>5</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>651.989990</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1545 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 tweet  \\\n",
       "date                                                                     \n",
       "2016-08-23 16:00:00  [Journalist Q&amp;A for 30 mins and embargo en...   \n",
       "2016-08-30 16:00:00  [Thanks for the longstanding faith in SpaceX. ...   \n",
       "2016-08-31 16:00:00  [@Lockyep Not allowed, according to HK regulat...   \n",
       "2016-09-01 16:00:00  [Loss of Falcon vehicle today during propellan...   \n",
       "2016-09-02 09:30:00  [Finishing Autopilot blog postponed to  end of...   \n",
       "...                                                                ...   \n",
       "2021-07-16 09:30:00  [@AaronS5_ @FrenchieEAP @karpathy Yes, @Austin...   \n",
       "2021-07-16 16:00:00  [@TeslaNY Do you even press?, @Teslarati Impro...   \n",
       "2021-07-19 09:30:00  [@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...   \n",
       "2021-07-19 16:00:00  [@jack @BitcoinMagazine @CathieDWood Sure, I h...   \n",
       "2021-07-20 09:30:00  [@TLPN_Official @SpaceX Depending on progress ...   \n",
       "\n",
       "                     num_tweets  username       price  percent change ticker  \n",
       "date                                                                          \n",
       "2016-08-23 16:00:00           2  elonmusk   44.967999        0.002318   TSLA  \n",
       "2016-08-30 16:00:00           3  elonmusk   42.268002       -0.022072    NaN  \n",
       "2016-08-31 16:00:00           5  elonmusk   42.402000        0.007508   TSLA  \n",
       "2016-09-01 16:00:00           1  elonmusk   40.153999       -0.039424   TSLA  \n",
       "2016-09-02 09:30:00           2  elonmusk   40.466000        0.007770   TSLA  \n",
       "...                         ...       ...         ...             ...    ...  \n",
       "2021-07-16 09:30:00           4  elonmusk  654.679993        0.006271   TSLA  \n",
       "2021-07-16 16:00:00           4  elonmusk  644.219971       -0.015977   TSLA  \n",
       "2021-07-19 09:30:00          31  elonmusk  629.890015       -0.022244    NaN  \n",
       "2021-07-19 16:00:00           2  elonmusk  646.219971        0.025925   TSLA  \n",
       "2021-07-20 09:30:00           5  elonmusk  651.989990        0.008929   TSLA  \n",
       "\n",
       "[1545 rows x 6 columns]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>username</th>\n",
       "      <th>price</th>\n",
       "      <th>percent change</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-07-16 16:00:00</th>\n",
       "      <td>[@TeslaNY Do you even press?, @Teslarati Impro...</td>\n",
       "      <td>4</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>644.219971</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-19 09:30:00</th>\n",
       "      <td>[@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...</td>\n",
       "      <td>31</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>629.890015</td>\n",
       "      <td>-0.022244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 tweet  \\\n",
       "date                                                                     \n",
       "2021-07-16 16:00:00  [@TeslaNY Do you even press?, @Teslarati Impro...   \n",
       "2021-07-19 09:30:00  [@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...   \n",
       "\n",
       "                     num_tweets  username       price  percent change ticker  \n",
       "date                                                                          \n",
       "2021-07-16 16:00:00           4  elonmusk  644.219971       -0.015977   TSLA  \n",
       "2021-07-19 09:30:00          31  elonmusk  629.890015       -0.022244    NaN  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['2021-07-16 16:00:00': '2021-07-19 09:30:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!SNIP!!######\n",
    "####################\n",
    "def snip1(df):\n",
    "    df = df.set_index('date')\n",
    "    combined = pd.DataFrame()\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df['price'].iloc[i] == 0:\n",
    "            combined = df.iloc[i]\n",
    "            j = i + 1\n",
    "            while df['price'].iloc[j] == 0:\n",
    "                combined += df.iloc[j]\n",
    "                j += 1\n",
    "            username = df.iloc[j]['username']\n",
    "            price = df.iloc[j]['price']\n",
    "            percent_change = df.iloc[j]['percent change']\n",
    "            df.iloc[j] += combined\n",
    "            df['username'].iloc[j] = username\n",
    "            df['price'].iloc[j] = price\n",
    "            df['percent change'].iloc[j] = percent_change\n",
    "            i = j\n",
    "        i += 1\n",
    "    df = df[df['price'] !=0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>username</th>\n",
       "      <th>price</th>\n",
       "      <th>percent change</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-08-23 16:00:00</td>\n",
       "      <td>[Journalist Q&amp;amp;A for 30 mins and embargo en...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>44.967999</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-08-28 09:30:00</td>\n",
       "      <td>[@Kotaku one of my favorite games as a kid, @B...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>44.162666</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-08-30 16:00:00</td>\n",
       "      <td>[Thanks for the longstanding faith in SpaceX. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>42.268002</td>\n",
       "      <td>-0.022072</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-08-31 16:00:00</td>\n",
       "      <td>[@Lockyep Not allowed, according to HK regulat...</td>\n",
       "      <td>5</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>42.402000</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-09-01 16:00:00</td>\n",
       "      <td>[Loss of Falcon vehicle today during propellan...</td>\n",
       "      <td>1</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>40.153999</td>\n",
       "      <td>-0.039424</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>2021-07-18 09:30:00</td>\n",
       "      <td>[@ArtifactsHub And all-time hodl champion, @Ar...</td>\n",
       "      <td>6</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>638.153341</td>\n",
       "      <td>-0.010441</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>2021-07-18 16:00:00</td>\n",
       "      <td>[@thePiggsBoson Problem 1st, theory 2nd is for...</td>\n",
       "      <td>1</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>645.553304</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>2021-07-19 09:30:00</td>\n",
       "      <td>[@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...</td>\n",
       "      <td>3</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>629.890015</td>\n",
       "      <td>-0.024263</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>2021-07-19 16:00:00</td>\n",
       "      <td>[@jack @BitcoinMagazine @CathieDWood Sure, I h...</td>\n",
       "      <td>2</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>646.219971</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>2021-07-20 09:30:00</td>\n",
       "      <td>[@TLPN_Official @SpaceX Depending on progress ...</td>\n",
       "      <td>5</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>651.989990</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2177 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                              tweet  \\\n",
       "22   2016-08-23 16:00:00  [Journalist Q&amp;A for 30 mins and embargo en...   \n",
       "23   2016-08-28 09:30:00  [@Kotaku one of my favorite games as a kid, @B...   \n",
       "24   2016-08-30 16:00:00  [Thanks for the longstanding faith in SpaceX. ...   \n",
       "25   2016-08-31 16:00:00  [@Lockyep Not allowed, according to HK regulat...   \n",
       "26   2016-09-01 16:00:00  [Loss of Falcon vehicle today during propellan...   \n",
       "...                  ...                                                ...   \n",
       "2194 2021-07-18 09:30:00  [@ArtifactsHub And all-time hodl champion, @Ar...   \n",
       "2195 2021-07-18 16:00:00  [@thePiggsBoson Problem 1st, theory 2nd is for...   \n",
       "2196 2021-07-19 09:30:00  [@DragTimes @Tesla Nice, @grimnut @Tesla @Whol...   \n",
       "2197 2021-07-19 16:00:00  [@jack @BitcoinMagazine @CathieDWood Sure, I h...   \n",
       "2198 2021-07-20 09:30:00  [@TLPN_Official @SpaceX Depending on progress ...   \n",
       "\n",
       "      num_tweets  username       price  percent change ticker  \n",
       "22             2  elonmusk   44.967999        0.002318   TSLA  \n",
       "23             2  elonmusk   44.162666        0.011081   TSLA  \n",
       "24             1  elonmusk   42.268002       -0.022072   TSLA  \n",
       "25             5  elonmusk   42.402000        0.007508   TSLA  \n",
       "26             1  elonmusk   40.153999       -0.039424   TSLA  \n",
       "...          ...       ...         ...             ...    ...  \n",
       "2194           6  elonmusk  638.153341       -0.010441   TSLA  \n",
       "2195           1  elonmusk  645.553304        0.011596   TSLA  \n",
       "2196           3  elonmusk  629.890015       -0.024263   TSLA  \n",
       "2197           2  elonmusk  646.219971        0.025925   TSLA  \n",
       "2198           5  elonmusk  651.989990        0.008929   TSLA  \n",
       "\n",
       "[2177 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangled_df[wrangled_df['username'] == 'elonmusk']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = snip1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2016-08-23 16:00:00    2\n",
       "2016-08-28 09:30:00    2\n",
       "2016-08-30 16:00:00    1\n",
       "2016-08-31 16:00:00    5\n",
       "2016-09-01 16:00:00    1\n",
       "                      ..\n",
       "2021-07-18 09:30:00    6\n",
       "2021-07-18 16:00:00    1\n",
       "2021-07-19 09:30:00    3\n",
       "2021-07-19 16:00:00    2\n",
       "2021-07-20 09:30:00    5\n",
       "Name: num_tweets, Length: 2177, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['num_tweets'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!REST!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!FULL!!######\n",
    "####################\n",
    "\n",
    "def fix_closed_market_tweets(test):\n",
    "    test = test.dropna()\n",
    "    test.reset_index(inplace=True, drop=True)\n",
    "    test = test.set_index('date')\n",
    "    for i in range(len(test)):\n",
    "        test['tweet'].iloc[i] += \" \"\n",
    "\n",
    "    i = 0\n",
    "    combined=pd.DataFrame()\n",
    "\n",
    "    while i < len(test):\n",
    "        if test['price'].iloc[i] == 0:\n",
    "            combined = test.iloc[i]\n",
    "            j = i + 1\n",
    "            while test['price'].iloc[j] == 0:\n",
    "                combined += test.iloc[j]\n",
    "                j += 1\n",
    "            username = test.iloc[j]['username']\n",
    "            price = test.iloc[j]['price']\n",
    "            percent_change = test.iloc[j]['percent change']\n",
    "            test.iloc[j] += combined\n",
    "            test['username'].iloc[j] = username\n",
    "            test['price'].iloc[j] = price\n",
    "            test['percent change'].iloc[j] = percent_change\n",
    "            i = j\n",
    "        i += 1\n",
    "    test = test[test['price'] !=0]\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "######!!WORK!!######\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_df = wrangled_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'num_tweets', 'username', 'price', 'percent change', 'ticker'], dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='date'>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrangled_df[wrangled_df['username'] == 'elonmusk']['num_tweets'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis/Cleaning/and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Corpus\n",
    "tweet_corpus = list(ceos['tweet'].values)\n",
    "\n",
    "# tokenize\n",
    "texts = [[text for text in doc.split()] for doc in tweet_corpus]\n",
    "\n",
    "# Dictionary\n",
    "dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns\n",
    "import re\n",
    "import html \n",
    "import demoji\n",
    "from collections import Counter\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import contractions\n",
    "\n",
    "demoji.download_codes()\n",
    "EMAIL_PATTERN = r'[A-Za-z0-9]+[@]+[^\\s]+'\n",
    "MENTION_PATTERN = r'[@]+[^\\s]+'\n",
    "HASHTAG_PATTERN = r'[#]\\S+'\n",
    "LINK_PATTERN = r'https?:\\S+'\n",
    "CASHTAG_PATTERN = r'[$][A-Z]+'\n",
    "EMOJIS_PATTERN = r'((:\\))|(;\\)))'\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.union(set(['s', 'it', 'm', 'you', 're', 'it ', ' s', 's ', 'i', 't', 'week', 'month', 'year', 'day']))\n",
    "\n",
    "def engineer_tweet_features(wrangled_df, method='count'):\n",
    "    \"\"\"\n",
    "    Takes in raw tweet and extracts mentions, hashtags, links,\n",
    "    (currently only counts, will look into scikit learn before implementing my own code for future transformations)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']] = 0\n",
    "    features_df = wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']] \n",
    "    features_df['emails'] = wrangled_df['tweet'].apply(lambda x: re.findall(EMAIL_PATTERN, x)).apply(lambda x: len(x))\n",
    "    features_df['links'] = wrangled_df['tweet'].apply(lambda x: re.findall(LINK_PATTERN, x)).apply(lambda x: len(x))\n",
    "    features_df['mentions'] = wrangled_df['tweet'].apply(lambda x: re.findall(MENTION_PATTERN, x)).apply(lambda x: len(x))\n",
    "    features_df['hashtags'] = wrangled_df['tweet'].apply(lambda x: re.findall(HASHTAG_PATTERN, x)).apply(lambda x: len(x))\n",
    "    features_df['emojis'] = wrangled_df['tweet'].apply(lambda x: demoji.findall_list(x)).apply(lambda x: len(x))\n",
    "    return features_df\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import string\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import normalize\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 variety=\"BrE\",\n",
    "                 user_abbrevs={},\n",
    "                 n_jobs=1):\n",
    "        \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \n",
    "        variety - format of date (AmE - american type, BrE - british format) \n",
    "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
    "        n_jobs - parallel jobs to run\n",
    "        \"\"\"\n",
    "        self.username = userna,e\n",
    "        self.user_abbrevs = user_abbrevs\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        partitions = 1\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "            partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "            return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "            partitions = min(self.n_jobs, cores)\n",
    "\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        normalized_text = self._normalize(text)\n",
    "        doc = nlp(normalized_text)\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        # some issues in normalise package\n",
    "        try:\n",
    "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])\n",
    "\n",
    "def clean_text(sample_tweets, stop_words=None):\n",
    "    \"\"\"\n",
    "    add normalize, do some exploring to see if any abbreviations need to be used\n",
    "    https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a\n",
    "    \"\"\"\n",
    "    new_sample_tweets = html.unescape(sample_tweets)\n",
    "    new_sample_tweets = ' '.join(re.sub(EMAIL_PATTERN, ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(MENTION_PATTERN, ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(HASHTAG_PATTERN, ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(CASHTAG_PATTERN, ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(LINK_PATTERN, ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(r'(w/)', 'with ', new_sample_tweets).split())\n",
    "    new_sample_tweets = demoji.replace(new_sample_tweets, repl=' ')\n",
    "    new_sample_tweets = ' '.join(re.sub(r'((:\\)))', 'smiling face', new_sample_tweets).split())\n",
    "    new_sample_tweets = ' '.join(re.sub(r'((;\\)))', 'winking face', new_sample_tweets).split())\n",
    "    new_sample_tweets = new_sample_tweets.lower()\n",
    "    new_sample_tweets = new_sample_tweets.replace('/', '')\n",
    "    new_sample_tweets = new_sample_tweets.replace('\\\\', '')\n",
    "    new_sample_tweets = ' '.join([contractions.fix(word) for word in new_sample_tweets.split()])\n",
    "    new_sample_tweets = ' '.join(re.sub(r'[^a-z]', ' ', new_sample_tweets).split())\n",
    "    new_sample_tweets = PorterStemmer().stem(new_sample_tweets) #\n",
    "    new_sample_tweets = ' '.join([word for word in new_sample_tweets.split() if len(word) >2 ])\n",
    "    if stop_words !=None:\n",
    "        new_sample_tweets = ' '.join([word for word in new_sample_tweets.split() if word not in stop_words]) # Go through this\n",
    "    return new_sample_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 1.68 s, total: 1min 14s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# clean up and add counts of twitter tags\n",
    "wrangled_df['cleaned'] = wrangled_df['tweet']\n",
    "wrangled_df['cleaned'] = wrangled_df['tweet'].apply(lambda x: clean_text(x))\n",
    "wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']] = engineer_tweet_features(wrangled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data to play with\n",
    "X, y = wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']], wrangled_df['percent change']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "In this exercise you will create a Word2Vec model using Keras.\n",
    "\n",
    "The corpus used to pre-train the model is the script of all episodes of the The Big Bang Theory TV show, divided sentence by sentence. It is available in the variable bigbang.\n",
    "\n",
    "The text on the corpus was transformed to lower case and all words were tokenized. The result is stored in the tokenized_corpus variable.\n",
    "\n",
    "A Word2Vec model was pre-trained using a window size of 10 words for context (5 before and 5 after the center word), words with less than 3 occurrences were removed and the skip gram model method was used with 50 dimension. The model is saved on the file bigbang_word2vec.model.\n",
    "\n",
    "The class Word2Vec is already loaded in the environment from gensim.models.word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Train the model\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=embedding_dim, window=neighbor_words_num, iter=100)\n",
    "\n",
    "# Get top 3 similar words to 'captain'\n",
    "w2v_model.wv.most_similar(['captain'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "\n",
    "# instatiate the model\n",
    "ft_model = fasttext.FastText(size=embedding_dim, window=neighbor_words_num)\n",
    "\n",
    "# Build vocabulary\n",
    "ft_model.build_vocab(sentences=tokenized_corpus)\n",
    "\n",
    "# train the model\n",
    "ft_model.train(sentences=tokenized_corpus, total_examples=len(tokenized_corpus), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_df['target'] = wrangled_df['percent change'].apply(lambda x: 1 if x >= 0.005 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9238\n",
       "1    2559\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangled_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "X, y = X, y = wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']], wrangled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.592141       0.005504       0.575732      0.002577\n",
      "1        0.600751       0.007511       0.580254      0.007691\n",
      "2        0.613030       0.005498       0.585576      0.008279\n",
      "3        0.614671       0.005419       0.582568      0.005032\n",
      "4        0.616571       0.005108       0.584612      0.006766\n",
      "CPU times: user 124 ms, sys: 14.6 ms, total: 139 ms\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params = {'objective':'reg:logistic', 'max_depth':3}\n",
    "\n",
    "# num_boost_round = number of trees to build\n",
    "cv_results = xgb.cv(dtrain=train_dmatrix, params=params, nfold=3, num_boost_round=5, metrics='auc', as_pandas=True, seed=0)\n",
    "#print(cv_results)\n",
    "#cv_results['accuracy'] = 1 - cv_results['test-error-mean']\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49864718614718617\n",
      "[[1843    5]\n",
      " [ 512    0]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=0)\n",
    "xg_cl.fit(X_train, y_train)\n",
    "y_pred = xg_cl.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "nmf = NMF(n_components=20, random_state=0, init='nndsvd')\n",
    "vect = TfidfVectorizer(min_df = 3, max_df = 0.85)\n",
    "csr_matrix = vect.fit_transform(wrangled_df['cleaned'])\n",
    "factors = nmf.fit_transform(csr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10242"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(nmf.components_, columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     65.588279\n",
       "14    63.998729\n",
       "9     37.727167\n",
       "15    36.803168\n",
       "0     35.150401\n",
       "13    33.912393\n",
       "5     31.238873\n",
       "8     31.027398\n",
       "17    26.503978\n",
       "10    25.275397\n",
       "12    24.661935\n",
       "4     23.283768\n",
       "19    21.878420\n",
       "1     20.475386\n",
       "7     17.987849\n",
       "16    16.856223\n",
       "11    16.042645\n",
       "6     13.021920\n",
       "2     10.086956\n",
       "18     6.368625\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tesla         1.363440\n",
       "yes           0.722400\n",
       "but           0.464052\n",
       "car           0.450761\n",
       "model         0.400234\n",
       "haha          0.390790\n",
       "production    0.301104\n",
       "cars          0.284971\n",
       "than          0.269163\n",
       "next          0.263617\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.iloc[3].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "CPU times: user 12min 42s, sys: 1min, total: 13min 42s\n",
      "Wall time: 13min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Corpus\n",
    "tweet_corpus = list(ceos['tweet'].values)\n",
    "\n",
    "# tokenize\n",
    "texts = [[text for text in doc.split()] for doc in tweet_corpus]\n",
    "\n",
    "# Dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "# Use Gensim's NMF to get the best num of topics via coherence score\n",
    "\n",
    "# Create a dictionary\n",
    "\n",
    "# Filter out extremes to limit the number of features\n",
    "dictionary.filter_extremes(\n",
    "    no_below=3,\n",
    "    no_above=0.85,\n",
    "    keep_n=5000\n",
    ")\n",
    "\n",
    "# Create the bag-of-words format (list of (token_id, token_count))\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create a list of the topic numbers we want to try\n",
    "topic_nums = list(np.arange(5, 75 + 1, 5))\n",
    "\n",
    "# Run the nmf model and calculate the coherence score\n",
    "# for each number of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num in topic_nums:\n",
    "    nmf = Nmf(\n",
    "        corpus=corpus,\n",
    "        num_topics=num,\n",
    "        id2word=dictionary,\n",
    "        chunksize=2000,\n",
    "        passes=5,\n",
    "        kappa=.1,\n",
    "        minimum_probability=0.01,\n",
    "        w_max_iter=300,\n",
    "        w_stop_condition=0.0001,\n",
    "        h_max_iter=100,\n",
    "        h_stop_condition=0.001,\n",
    "        eval_every=10,\n",
    "        normalize=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run the coherence model to get the score\n",
    "    cm = CoherenceModel(\n",
    "        model=nmf,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    coherence_scores.append(round(cm.get_coherence(), 5))\n",
    "\n",
    "# Get the number of topics with the highest coherence score\n",
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, reverse=True)[0][0]\n",
    "\n",
    "print(best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, reverse=True)[0][0]\n",
    "\n",
    "print(best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.54752),\n",
       " (10, 0.51356),\n",
       " (15, 0.52321),\n",
       " (20, 0.52289),\n",
       " (25, 0.50343),\n",
       " (30, 0.50165),\n",
       " (35, 0.48579),\n",
       " (40, 0.47474),\n",
       " (45, 0.46916),\n",
       " (50, 0.46047),\n",
       " (55, 0.44977),\n",
       " (60, 0.45063),\n",
       " (65, 0.43021),\n",
       " (70, 0.42988),\n",
       " (75, 0.42049)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# currently testing these features out as counts, but adding them (some/all) into the nlp pipeline when later developed\n",
    "wrangled_df[['emails', 'links', 'mentions', 'hashtags', 'emojis']] = 0\n",
    "\n",
    "wrangled_df['emails'] = wrangled_df['tweet'].apply(lambda x: re.findall(EMAIL_PATTERN, x)).apply(lambda x: len(x))\n",
    "wrangled_df['links'] = wrangled_df['tweet'].apply(lambda x: re.findall(LINK_PATTERN, x)).apply(lambda x: len(x))\n",
    "wrangled_df['mentions'] = wrangled_df['tweet'].apply(lambda x: re.findall(MENTION_PATTERN, x)).apply(lambda x: len(x))\n",
    "wrangled_df['hashtags'] = wrangled_df['tweet'].apply(lambda x: re.findall(HASHTAG_PATTERN, x)).apply(lambda x: len(x))\n",
    "wrangled_df['emojis'] = wrangled_df['tweet'].apply(lambda x: demoji.findall_list(x)).apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_df['emojis'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new_sample_tweets.split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words += [most_common[i][0] for i in range(len(most_common))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words += ['not', 'up', 'we', 'pls', 'm', 'an', 'if', 'do']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_tweets = ' '.join([w for w in new_sample_tweets.split() if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(new_sample_tweets.split())\n",
    "word_freq = {}\n",
    "for word in new_sample_tweets.split():\n",
    "    if word in word_freq.keys():\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_tweets = sample_tweets.join(re.sub())\n",
    "\n",
    "# mentions (@)\n",
    "# hashtags(#)\n",
    "# urls\n",
    "# encoding\n",
    "# emojis\n",
    "# punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "html.unescape(sample_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweets(full_df):\n",
    "    \"\"\"\n",
    "        takes in a pandas dataframe of tweets with dates and times for each created tweet, and the tweet author's handle.\n",
    "        It groups all of the user's tweets that occured between opening (9:30:00 EST) and closing (16:00:00 EST) stock times for open stock days, and returns a dataframe of the grouped tweets and the number of tweets\n",
    "        that were collected for that time frame.\n",
    "        A tweet is grouped via the following criteria:\n",
    "        - If tweet is earlier than 9:30, it applies to that price (opening) on the same date. \n",
    "        - It tweet is after 9:30, but before 16:00 (closing), it applies to the following price on the same date.\n",
    "        - If tweet is after 16:00, it applies to the next opening date.\n",
    "\n",
    "        To Do (not in order):\n",
    "        collect tweets and stocks directly \n",
    "        allow to specify period\n",
    "\n",
    "        \"\"\"\n",
    "    categorical_features = ['urls', 'photos', 'hashtags', 'cashtags', 'thumbnail', 'mentions']\n",
    "    text_features = ['tweet']\n",
    "    timestamp_features = ['date', 'time']\n",
    "    users = full_df.username.unique()\n",
    "    merged_df = pd.DataFrame()\n",
    "    def combine_tweets_user(df):\n",
    "        username = df.username\n",
    "        collected_tweets = {}\n",
    "        #df['time'] = pd.to_datetime(df['time'])\n",
    "        #df['number of tweets'] = 1\n",
    "        # fix missing vals\n",
    "        df[categorical_features] = df[categorical_features].applymap(lambda x: np.nan if len(x) == 0 else x)\n",
    "        # If tweet is earlier than 9:30, it applies to that price (opening) on the same date. \n",
    "\n",
    "        df['time'] = pd.to_datetime(df['created_at']).dt.time\n",
    "        df['date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            if df['time'].iloc[i] <= dt.strptime('09:30:00', '%H:%M:%S').time():\n",
    "                df['time'].iloc[i] = dt.strptime('09:30:00', '%H:%M:%S').time()\n",
    "\n",
    "        # It tweet is after 9:30, but before 16:00 (closing), it applies to the following price on the same date.\n",
    "\n",
    "            if (df['time'].iloc[i] > dt.strptime('09:30:00', '%H:%M:%S').time()) and (df['time'].iloc[i] <= dt.strptime('16:00:00', '%H:%M:%S').time()):\n",
    "                df['time'].iloc[i] = dt.strptime('16:00:00', '%H:%M:%S').time()\n",
    "\n",
    "        # If tweet is after 16:00, apply it to the next opening date.\n",
    "        #for i in range(len(df)):\n",
    "            if df['time'].iloc[i] > dt.strptime('16:00:00', '%H:%M:%S').time():\n",
    "                df['date'].iloc[i] = df['date'].iloc[i] + timedelta(days=1)\n",
    "                df['time'].iloc[i] = dt.strptime('09:30:00', '%H:%M:%S').time()\n",
    "\n",
    "        # Combine dates and times\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        df['time'] = df['time'].astype(str)\n",
    "        df['date'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "\n",
    "        tweet = \"\"\n",
    "        df['number of tweets'] = 1\n",
    "        to_merge = df.groupby('date').sum()\n",
    "        #to_merge['date'] = pd.to_datetime(to_merge['date'], format='%Y-%M-%d').dt.date\n",
    "        # date is dictionary key\n",
    "        collected_tweets[df['date'].iloc[0]] = tweet\n",
    "\n",
    "        for i in range(len(df.index)):\n",
    "            current_date = df['date'].iloc[i]\n",
    "            if current_date in collected_tweets:\n",
    "                collected_tweets[current_date] += \" \" + str(df['tweet'].iloc[i])\n",
    "            else:\n",
    "                collected_tweets[current_date] = str(df['tweet'].iloc[i])\n",
    "\n",
    "        df = pd.DataFrame.from_dict(collected_tweets, orient='index', columns = ['tweet'])\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.rename(columns={'index':'date'})\n",
    "        df['username'] = username\n",
    "        df_merged = pd.merge(df, to_merge.reset_index(), on='date')\n",
    "\n",
    "        return df_merged\n",
    "    for user in users:\n",
    "        merged_df = merged_df.append(combine_tweets_user(full_df[full_df['username'] == user]))\n",
    "    return merged_df[['username', 'tweet', 'date', 'number of tweets']]\n",
    "    \n",
    "def combine_tweets_stocks(ceos_merged, stocks_full):\n",
    "    new_df = pd.DataFrame()\n",
    "    for user in ceos_merged['username'].unique():\n",
    "        ticker = handles_tickers[user]\n",
    "        tweet_df = ceos_merged[ceos_merged['username'] == user]\n",
    "        stock_df = stocks_full[stocks_full['ticker'] == ticker]\n",
    "        new_df = new_df.append(tweet_df.merge(stock_df, how='left', on='date')).dropna(subset=['ticker'])  \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test\n",
    "ceos_merged = combine_tweets(ceos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ceos_merged['date'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceos_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stocks_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceos_merged['username'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "for user in ceos_merged['username'].unique():\n",
    "    ticker = handles_tickers[user]\n",
    "    tweet_df = ceos_merged[ceos_merged['username'] == user]\n",
    "    stock_df = stocks_full[stocks_full['ticker'] == ticker]\n",
    "    new_df = new_df.append(tweet_df.merge(stock_df, how='left', on='date'))\n",
    "    new_df = new_df.dropna(subset=['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEOS_list = list(CEOS.keys())\n",
    "stocks_list = list(stocks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CEOS_list)):\n",
    "    CEOS[CEOS_list[i]] = CEOS[CEOS_list[i]].merge(stocks[stocks_list[i]], how='left', on='date')\n",
    "    CEOS[CEOS_list[i]] = CEOS[CEOS_list[i]].sort_values(by = 'date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "object_features = ceos.dtypes[ceos.dtypes == 'object'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musk_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_stocks(stock): #Here\n",
    "\n",
    "        # Instatiate Open and Close\n",
    "        stock_open = stock[['date','open']]\n",
    "        stock_close = stock[['date','close']]\n",
    "\n",
    "        # Convert dates to datetime objects\n",
    "        stock_open['date'] = pd.to_datetime(stock_open['date'])\n",
    "        stock_close['date'] = pd.to_datetime(stock_close['date'])\n",
    "\n",
    "        # Convert datetimes into datetime string format\n",
    "        stock_open['date'] = stock_open['date'].dt.strftime('%Y-%m-%d 09:30:00')\n",
    "        stock_close['date'] = stock_close['date'].dt.strftime('%Y-%m-%d 16:00:00')\n",
    "\n",
    "        # Convert strings back into datetime objects\n",
    "        stock_open['date'] = pd.to_datetime(stock_open['date'])\n",
    "        stock_close['date'] = pd.to_datetime(stock_close['date'])\n",
    "\n",
    "        # Get earliest and latest stock price dates to create a date index\n",
    "        stock_open['price'] = stock_open['open']\n",
    "        stock_open.drop('open', axis=1, inplace=True)\n",
    "\n",
    "        stock_close['price'] = stock_close['close']\n",
    "        stock_close.drop('close', axis=1, inplace=True)\n",
    "\n",
    "        start_date_open = dt.strftime(stock_open.reset_index().date.min(), '%Y-%m-%d %H:%M:%S')\n",
    "        end_date_open = dt.strftime(stock_open.reset_index().date.max(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        start_date_close = dt.strftime(stock_close.reset_index().date.min(), '%Y-%m-%d %H:%M:%S')\n",
    "        end_date_close = dt.strftime(stock_close.reset_index().date.max(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        date_indx_open = pd.date_range(start_date_open, end_date_open).tolist()\n",
    "        date_indx_close = pd.date_range(start_date_close, end_date_close).tolist()\n",
    "        date_indx_open = pd.Series(date_indx_open, name='date')\n",
    "        date_indx_close = pd.Series(date_indx_close, name='date')\n",
    "\n",
    "        # Merge date index onto stock dataframes\n",
    "        stock_open = pd.merge(date_indx_open, stock_open, how='left')\n",
    "        stock_close = pd.merge(date_indx_close, stock_close, how='left')\n",
    "\n",
    "        # Interpolate missing values\n",
    "        stock_open['price'].interpolate(method='linear', inplace=True)\n",
    "        stock_close['price'].interpolate(method='linear', inplace=True)\n",
    "        \n",
    "        # MAKE SURE YOU CAN UNDERSTAND THIS\n",
    "\n",
    "        # Reset index and join open and close dataframes together\n",
    "        stock_open.set_index('date', inplace=True)\n",
    "        stock_close.set_index('date', inplace=True)\n",
    "\n",
    "        stock = pd.concat([stock_open, stock_close])\n",
    "        \n",
    "        stock.sort_index(inplace=True)\n",
    "        \n",
    "        return stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEOS = {}\n",
    "for user in usernames:\n",
    "    CEOS[user] = ceos[ceos['username'] == user]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(CEOS['elonmusk']['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "# Get tweets that aren't replies\n",
    "    CEOS[CEO]['reply_length'] = -1\n",
    "    for i in range(len(CEOS[CEO])):\n",
    "        CEOS[CEO]['reply_length'].loc[i] = len(CEOS[CEO]['reply_to'].loc[i])\n",
    "\n",
    "    CEOS[CEO] = CEOS[CEO][CEOS[CEO]['reply_length'] == 0]\n",
    "    CEOS[CEO].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tweets = pd.DataFrame()\n",
    "#for CEO in CEOS:\n",
    "#    all_tweets = all_tweets.append(CEOS[CEO])\n",
    "    \n",
    "#all_tweets.to_pickle(f'/Users/tylerpoore/Workspace/Spring Board/Projects/Capstone_notebooks/Capstone_1_Final/data/ceos.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing and Cleaning the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining The Tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the scraped data\n",
    "\n",
    "for CEO in CEOS:\n",
    "    print(\"================\")\n",
    "    print(CEO)\n",
    "    print(\"================\")\n",
    "    print(CEOS[CEO][['date','tweet']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw twitter data has mentions, urls, emojis, and other languages besides english at first glance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the tweet columns\n",
    "\n",
    "CEOS['elonmusk'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataframe columns entries, there are no missing values. That may be because they are using a different character than NaN, and I will look into this later. Most are of type object, but there are 4 integer columns: user_id; reply, retweet, and like counts; and video for some odd reason; and 1 boolean retweet column. All of the columns are the same type. Let's examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at sample integer columns\n",
    "\n",
    "CEOS['elonmusk'].select_dtypes(include=['int']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first set of object columns\n",
    "\n",
    "CEOS['elonmusk'].select_dtypes(include=['object']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these columns I will exclude as they likely won't contribute any contributable features to predicting the stock price. I will determine the language after cleaning the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEOS['elonmusk'][['tweet', 'mentions', 'urls', 'photos','hashtags', 'cashtags', 'quote_url']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values are represented by empty lists. Let's turn this into a better represenation of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the languages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print(CEOS[CEO]['language'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what und is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEOS['elonmusk']['tweet'][CEOS['elonmusk']['language'] == 'und']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like und occurs when there are a lot of mentions, links, or emojis, so I will keep this and clean them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the en and und languages \n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO] = CEOS[CEO][(CEOS[CEO]['language'] == 'en') | (CEOS[CEO]['language'] == 'und')]\n",
    "    CEOS[CEO].reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print(CEOS[CEO]['language'].value_counts())\n",
    "    print(CEOS[CEO]['language'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Converting to Missing Values and Categorical Features <a id='1.3.2_Missing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    CEOS[CEO][['mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'thumbnail']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert empty lists into NaNs for easier processing\n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO][['mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'thumbnail']] = CEOS[CEO][['mentions', 'urls', 'photos', 'hashtags','cashtags', 'thumbnail']].applymap(lambda x: np.nan if len(x) == 0 else x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print(CEOS[CEO][['mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'thumbnail']].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print('\\n')\n",
    "    print('missing values')\n",
    "    print('===============')\n",
    "    print(CEOS[CEO][['mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'thumbnail']].isna().sum())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categ = ['mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'thumbnail']\n",
    "for categorical in categ:\n",
    "    print(categorical.upper())\n",
    "    print(CEOS['JohnLegere'][categorical][~CEOS['JohnLegere'][categorical].isna()])\n",
    "    print('===========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Change categoricals to binary values\n",
    "for CEO in CEOS:\n",
    "    for categorical in categ:\n",
    "        CEOS[CEO][categorical][CEOS[CEO][categorical].isnull()] = 0\n",
    "\n",
    "for CEO in CEOS:\n",
    "    for categorical in categ:\n",
    "        for i in range(len(CEOS[CEO])):\n",
    "            if CEOS[CEO][categorical].iloc[i] != 0:\n",
    "                if categorical != 'thumbnail':\n",
    "                    CEOS[CEO][categorical].iloc[i] = len(CEOS[CEO][categorical].iloc[i])\n",
    "                else:\n",
    "                    CEOS[CEO]['thumbnail'].iloc[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    CEOS[CEO]['cashtags'] = CEOS[CEO]['cashtags'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for categorical in categ:\n",
    "    print(CEOS['elonmusk'][categorical].value_counts())\n",
    "    print('============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Select Feature Columns and Combine Tweets for Each Day <a id='1.3.3_Features'></a>\n",
    "\n",
    "I will be keeping most of the categorical and int features, but I won't be keeping the Cashtag column as it is empty for most of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set appropriate columns \n",
    "\n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO] = CEOS[CEO][[\n",
    "        'date', 'time', 'username', 'tweet', \n",
    "        'mentions', 'hashtags', 'video', 'photos', 'urls']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change some of the features to integers\n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will make a separate dataframe for these and append them to a dataframe of tweets that have been grouped by the date. Because I am looking how tweets impact the stock prices, I want to analyze tweets that happen before the price point. To this end I will be looking at the closing price, and tweets that occur on the same day after the market closes will be counted towards the following day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets = len(CEOS['elonmusk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Takes ~1 min\n",
    "# is preprocessing with dictionaries faster than with a pandas dataframe?\n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO] = combine_tweets(CEOS[CEO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert num_tweets == CEOS['elonmusk']['number of tweets'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print('Number of days collected: {}'.format(CEOS[CEO].shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Combining Stocks and Tweets DataFrames <a id='1.4_Combining_DataFrames'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge stocks on to tweets\n",
    "for stock in stocks:\n",
    "    stocks[stock].reset_index(inplace=True)\n",
    "    print(stock.upper)\n",
    "    print(stocks[stock].head(3))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEOS_list = list(CEOS.keys())\n",
    "stocks_list = list(stocks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CEOS_list)):\n",
    "    CEOS[CEOS_list[i]] = CEOS[CEOS_list[i]].merge(stocks[stocks_list[i]], how='left', on='date')\n",
    "    CEOS[CEOS_list[i]] = CEOS[CEOS_list[i]].sort_values(by = 'date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEO.upper())\n",
    "    print(CEOS[CEO].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Sort DataFrames and Clean <a id='1.4.1_Sorting_and_Cleaning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tweets that occur when market it closed and add to next market open.\n",
    "\n",
    "for CEO in CEOS:\n",
    "    CEOS[CEO] = fix_closed_market_tweets(CEOS[CEO])\n",
    "    CEOS[CEO].reset_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "for CEO in CEOS:\n",
    "    print(CEO)\n",
    "    print(CEOS[CEO].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the latest tweets are before the next market opens, some of them have missing values and I will have to remove them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    print(CEOS[CEO].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CEOS['elonmusk'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_columns = CEOS['elonmusk'].select_dtypes(include=['int']).columns\n",
    "floats_columns = CEOS['elonmusk'].select_dtypes(include=['float']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CEO in CEOS:\n",
    "    CEOS[CEO][ints_columns] = CEOS[CEO][ints_columns].astype(int)\n",
    "    CEOS[CEO][floats_columns] = CEOS[CEO][floats_columns].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 DataFrame Prices vs Original Stock Prices <a id='1.4.2_Looking_at_Original_Stocks'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look to see how well the full stock prices match the merged tweet and stock DataFrames\n",
    "for i in range(len(stocks_list)):\n",
    "    _ = plt.plot(stocks[stocks_list[i]]['date'], stocks[stocks_list[i]]['price'])\n",
    "    _ = plt.plot(CEOS[CEOS_list[i]]['date'], CEOS[CEOS_list[i]]['price'], c='red')\n",
    "    _ = plt.title(stocks_list[i])\n",
    "    _ = plt.xticks(rotation=60)\n",
    "    _ = plt.legend(['Stock Data', 'Stock Price with Tweets'])\n",
    "    _ = plt.show()\n",
    "    plt.savefig('./figures/data_wrangling/Price_Tweets_{}.png'.format(stocks_list[i]), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initalize a Figure and Axes\n",
    "for CEO in CEOS:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(CEOS[CEO]['date'], CEOS[CEO]['price'], color='blue')\n",
    "    plt.ylabel('price')\n",
    "    plt.title(f'{CEO} tweets with stock price')\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "    # Create a twin Axes that shares the x-axis\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    ax2.plot(CEOS[CEO]['date'], CEOS[CEO]['number of tweets'], color='g', alpha = 0.4)\n",
    "    plt.ylabel('number of tweets')\n",
    "    plt.legend(['number of tweets'])\n",
    "    plt.savefig('./figures/data_wrangling/num_tweets_stock_price_{}.png'.format(CEO), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CEOS['JohnLegere']\n",
    "test2 = CEOS['richardbranson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['date'] <= '2019']\n",
    "test2 = test2[test2['date'] >= '2020-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(test['date'], test['price'], color='blue')\n",
    "plt.ylabel('price')\n",
    "plt.title('John Legere tweets with stock price pre 2019')\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Create a twin Axes that shares the x-axis\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "\n",
    "ax2.plot(test['date'], test['number of tweets'], color='g', alpha = 0.4)\n",
    "plt.ylabel('number of tweets')\n",
    "plt.legend(['number of tweets'])\n",
    "plt.savefig('./figures/data_wrangling/num_tweets_stock_price_JohnLegere_pre2019', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(test2['date'], test2['price'], color='blue')\n",
    "plt.ylabel('price')\n",
    "plt.title('Richard Branson tweets with stock price post July 2020')\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Create a twin Axes that shares the x-axis\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "ax2.plot(test2['date'], test2['number of tweets'], color='g', alpha = 0.4)\n",
    "plt.ylabel('number of tweets')\n",
    "plt.legend(['number of tweets'])\n",
    "plt.savefig('./figures/data_wrangling/num_tweets_stock_price_richardbranson_post2020', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Saving Dataframes <a id='1.5_Exporting_DataFrames'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CEOS_list)):\n",
    "    CEOS[CEOS_list[i]].to_pickle(f'./data/{CEOS_list[i]}_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stocks = pd.DataFrame()\n",
    "for CEO in CEOS:\n",
    "    tweet_stocks = tweet_stocks.append(CEOS[CEO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_stocks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stocks.to_pickle(f'./data/tweet_stocks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ceos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the data types\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
